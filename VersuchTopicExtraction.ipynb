{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction in Newsfeeds\n",
    "* Autor: Prof. Dr. Johannes Maucher\n",
    "* Datum: 17.11.2015\n",
    "\n",
    "[Übersicht Ipython Notebooks im Data Mining Praktikum](Data Mining Praktikum.ipynb)\n",
    "\n",
    "# Einführung\n",
    "## Lernziele:\n",
    "\n",
    "In diesem Versuch sollen Kenntnisse in folgenden Themen vermittelt werden:\n",
    "\n",
    "* __RSS Feeds:__ Struktur von RSS Feeds analysieren und parsen mit dem _Universal Feed Parser_. \n",
    "* __Dokument Analyse:__ Die Häufigkeit aller Worte in einem Dokument (Inhalt des RSS Feeds) zählen und in einem Array verwalten. \n",
    "* __Merkmalsextraktion:__ Bestimmung von Merkmalen (hier auch: __Topics__) (Allgemein spricht man von Merkmalen. Im Fall, dass die NNMF auf Dokumente angewandt wird, werden die Merkmale auch mit __Topics__ oder __Themen__ bezeichnet) mit der \\emph{Non Negative Matrix Factorization}.\n",
    "* __Zuordnung__: Wie setzen sich die Topics aus den Wörtern zusammen? Wie stark sind die gefundenen Topics in den Artikeln vertreten?\n",
    "* __Dokument Clustering:__ Mit der NNMF kann auch ein Clustering realisiert werden. Jeder Topic repräsentiert ein Cluster. Jedes Dokument wird dem Cluster zugeordnet, dessen Topic am stärksten in ihm vertreten ist. \n",
    "\n",
    "Sämtliche Verfahren und Algorithmen werden in Python implementiert.\n",
    "\n",
    "## Theorie zur Vorbereitung\n",
    "\n",
    "Stellen Sie sich vor Sie möchten in eine eigene Webseite die RSS Feeds einer Menge von Nachrichtenservern einbinden. Da die unterschiedlichen Server wahrscheinlich Artikel zu den gleichen Themen anbieten, werden die Inhalte einiger Artikel ähnlich sein. Mit der __Nicht Negativen Matrixfaktorisierung (NNMF)__ kann für eine große Menge von Dokumenten eine Menge von Themen (Topics) ermittelt werden, auf die sich die Dokumente beziehen. Damit ist es u.a. möglich\n",
    "* die Dokumente thematisch zu ordnen\n",
    "* zu jedem Thema nur ein Dokument anzuzeigen\n",
    "\n",
    "### Ähnlichkeiten bestimmen und relevante Merkmale extrahieren\n",
    "\n",
    "Eine Sammlung von Dokumenten - in diesem Versuch die Menge aller Nachrichten der angegebenen Feeds - kann in einer Artikel/Wort-Matrix repräsentiert werden. Jede Zeile dieser Matrix gehört zu einem Dokument. Für jedes Wort, das mindestens in einem der Dokumente vorkommt, ist eine Spalte vorgesehen. Das Matrixelement in Zeile $i$, Spalte $j$ beschreibt wie häufig das Wort in Spalte $j$ im zur Zeile $i$ gehörenden Dokument vorkommt.\n",
    "\n",
    "Unter der Annahme, dass Artikel umso ähnlicher sind, je mehr Worte in diesen gemeinsam vorkommen, kann auf der Grundlage dieser Matrix die Ähnlichkeit zwischen den Artikeln berechnet werden. Hierzu könnte die Matrix z.B. einfach einem _Hierarchischen Clustering_ übergeben werden. Das hierarchische Clustering weist jedoch im Fall einer großen Menge von zu vergleichenden Objekten zwei wesentliche Nachteile auf: Erstens ist die wiederholte Berechnung der Distanzen zwischen allen Artikeln/Clustern extrem rechenaufwendig, zweitens ist die Darstellung einer großen Anzahl von Objekten im Dendrogramm nicht mehr übersichtlich. \n",
    "\n",
    "Für das Auffinden von Assoziationen zwischen Dokumenten hat sich in den letzten Jahren die Methode der __Nicht-Negativen Matrix Faktorisierung (NNMF)__ etabliert. Mit dieser Methode kann eine Menge von wesentlichen Merkmalen berechnet werden, anhand derer sich die Dokumente clustern lassen, d.h. Dokumente des gleichen Clusters repräsentieren das gleiche Merkmal (Thema). Ein solches Merkmal wird durch eine Menge von Worten beschrieben, z.B. $\\{$ _Paris, terror, IS_ $\\}$  oder $\\{$_refugee, syria, border_ $\\}$. Neben der Merkmalsextraktion stellt die relativ geringe Komplexität einen weiteren Vorteil der NNMF dar. Durch die Darstellung der Artikel/Wort-Matrix als Produkt von 2 Faktormatrizen müssen deutlich weniger Einträge gespeichert werden.\n",
    "\n",
    "### Nicht Negative Matrixfaktorisierung: Die Idee\n",
    "\n",
    "Die Artikel/Wort-Matrix wird im Folgenden mit $A$ bezeichnet. Sie besitzt $r$ Zeilen und $c$ Spalten, wobei $r$ die Anzahl der Artikel und $c$ die Anzahl der relevanten Worte in der Menge aller Artikel ist. Durch Multiplikation der Matrix $A$ mit dem Vektor $v$ (_wordvec_: Vektor der alle relevanten Worte enthält) werden die Worte den Artikeln $a$ (_articletitles_: Vektor der alle Artikeltitel enthält) zugeordnet:\n",
    "\n",
    "$$\n",
    "a=A*v.\n",
    "$$\n",
    "\n",
    "Die Idee der NNMF besteht darin die Matrix $A$ als Produkt zweier Matrizen $W$ und $H$ darzustellen,\n",
    "\n",
    "$$\n",
    "A=W*H\n",
    "$$\n",
    "\n",
    "wobei alle Elemente in $W$ und $H$ größer oder gleich Null sein müssen. Die Matrixmultiplikation erfordert, dass die Anzahl der Zeilen $m$ in $H$ gleich der Anzahl der Spalten in $W$ sein muss. \n",
    "Durch die Faktorisierung der Matrix $A$ wird die Zuordnung der Wörter des Wortvektors $v$  zu den Artikeln des Vektors $a$ in zwei Stufen zerlegt. \n",
    "\n",
    "$$\n",
    "f = H*v\n",
    "$$\n",
    "$$\n",
    "a = W*f \n",
    "$$\n",
    "\n",
    "In der ersten Stufe werden durch die Multiplikation von $v$ mit der Matrix $H$ die Wörter einem sogenannten Merkmalsvektor $f$ mit $m$ Elementen zugewiesen. In der zweiten Stufe werden durch die Multiplikation des Merkmalsvektor $f$ mit der Matrix $W$ die einzelnen Merkmale den Artikeln in $a$ zugeordnet. Die Matrix $H$ definiert also aus welchen Wörtern die Merkmale gebildet werden. Sie wird deshalb __Merkmalsmatrix__ genannt. Die Matrix $W$ hingegen beschreibt mit welchem Gewicht die einzelnen Merkmale in den verschiedenen Artikeln auftreten. Sie wird deshalb __Gewichtungsmatrix__ genannt.\n",
    "\n",
    "Daraus folgt: Wenn eine Faktorisierung der Matrix $A$ gefunden wird, dann werden damit auch relevante Merkmale, also die Themen, definiert, hinsichtlich derer die Artikel effizient kategorisiert werden. Durch die Matrixfaktorisierung wird eine __Merkmalsextraktion__ realisiert. \n",
    "\n",
    "### Berechnung der Matrixfaktoren\n",
    "\n",
    "Für die Berechnung der Faktoren wurde in [Lee, Algortihms for Non-negative Matrix Factorisation](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf) eine iterative Methode vorgestellt, die derzeit wohl am häufigsten angewandt wird und auch in dieser Übung implementiert werden soll. Der Algorithmus besteht aus folgenden Schritten:\n",
    "* Gebe die zu faktorisierende Matrix $A$ ein. $r$ sei die Anzahl der Zeilen und $c$ die Anzahl der Spalten von $A$.\n",
    "* Wähle die Anzahl $m$ der Merkmale, mit $m<c$. _Tipp:_ Für $m$ sollte zunächst ein Wert im Bereich $15$ bis $30$ gewählt werden.\n",
    "* Lege eine $m \\times c$ Matrix $H$ an mit initial zufälligen Elementen (Anwendung der numpy Funktion _random.random()_)\n",
    "* Lege eine $r \\times m$ Matrix $W$ an mit initial zufälligen Elementen (Anwendung der numpy Funktion _random.random()_)\n",
    "* Wiederhole bis maximale Anzahl der Iteration erreicht oder Kosten $k$ unter vordefinierter Schwelle:\n",
    "\n",
    "\t* Berechne aktuelles Produkt $B=W*H$ und bereche die Kostenfunktion \n",
    "\t\t$$\n",
    "\t\t\tk=\\left\\| A - B \\right\\|^2 = \\sum\\limits_{i,j} \\left(A_{i,j} - B_{i,j}\\right)^2\n",
    "\t\t$$ \n",
    "\t* Anpassung der Matrix $H$ durch folgende Neuberechnung der Matrixelemente\n",
    "    \n",
    "\t\t$$\n",
    "\t\tH_{i,j} := H_{i,j} \\frac{(W^T*A)_{i,j}}{(W^T*W*H)_{i,j}}\n",
    "\t\t$$\n",
    "        \n",
    "\t* __Nach__ der Anpassung der Matrix $H$: Anpassung der Matrix $W$ durch folgende Neuberechnung der Matrixelemente\n",
    "    \n",
    "\t\t$$\n",
    "\t\tW_{l,i} := W_{l,i} \\frac{(A*H^T)_{l,i}}{(W*H*H^T)_{l,i}}\n",
    "\t\t$$\n",
    "\n",
    "In [Lee, Algortihms for Non-negative Matrix Factorisation](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf) ist bewiesen, dass durch die o.g. Anpassungsroutinen die Kosten $k$ monoton abnehmen und in einem Minimum konvergieren. Der Algorithmus ist jedoch nicht optimal weil das gefundene Minimum ein lokales Minimum sein kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Vor dem Versuch zu klärende Fragen\n",
    " \n",
    " * Was versteht man unter Artikel/Wort-Matrix? Wie wird diese im aktuellen Versuch gebildet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "|      | Spalte   |  1.  |   2. |\n",
    "| ---- |:-------------:| -----:|\n",
    "|Zeile | Doclist | 'wort1' | 'wort2' |\n",
    "|   1.  | Doc1 |  1   |  0   | \n",
    "|   2.  | Doc2 |  0   |  0   | \n",
    "|   3.  | Doc3 |  0   |  0   |  \n",
    "\n",
    "Jede Zeile dieser Matrix gehört zu einem Dokument. Für jedes Wort, das mindestens in einem der Dokumente vorkommt, ist eine Spalte vorgesehen. Das Matrixelement in Zeile  i, Spalte  j beschreibt wie häufig das Wort in Spalte j im zur Zeile i gehörenden Dokument vorkommt.\n",
    "\n",
    " \n",
    "Gebildet wird diese Matrix in dem man eine bereinigte Liste aus allen Wörtern zu jedem Dokument erstellt wird. Zur Bereinigung zählen: Stoppwörter, Doppelte Wörter und Satzzeichen zu entfernen (Tokenizing). Außerdem das Normalisieren der Wortmenge. Diese Listen werden in einer Liste gespeichert.\n",
    "Anschließend wird aus diesem Objekt ein Gensim-Dictionary erzeugt. Es besitzt nun die Eigenschaft, dass es jedem relevanten Wort (Key) einen zufälligen, eindeutigen  Index (Value) erzeugt. \n",
    "\n",
    "\n",
    "Um die Performance zu erhöhen wird die Efficient Corpus Representation eingesetzt. Dabei wird aus dem Dictionary eine Liste aus Tupeln erzeugt, wobei der erste Wert, der Indize des Wortes ist und der zweite Wert die Häufigkeit des Wortes repräsentiert. Dies bildet den Corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wie multipliziert man die Matrix\n",
    "    $$\n",
    "    A= \\left( \\begin{array}{cccc}\n",
    "a_{00} & a_{01} & a_{02} & a_{03} \\\\ \n",
    "a_{10} & a_{11} & a_{12} & a_{13} \\\\ \n",
    "a_{20} & a_{21} & a_{22} & a_{23}\n",
    "\\end{array} \\right)\n",
    "    $$\n",
    "    mit dem Vektor  \n",
    "    $$\n",
    "    v=\\left( \\begin{array}{c}\n",
    "v_{0} \\\\ \n",
    "v_{1} \\\\ \n",
    "v_{2} \\\\ \n",
    "v_{3}\n",
    "\\end{array} \\right)\n",
    "    $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So multipliziert man die Matrix:\n",
    "    $$\n",
    "    a = A * v = \\left( \\begin{array}{cccc}\n",
    "a_{00} & a_{01} & a_{02} & a_{03} \\\\ \n",
    "a_{10} & a_{11} & a_{12} & a_{13} \\\\ \n",
    "a_{20} & a_{21} & a_{22} & a_{23}\n",
    "\\end{array} \\right)\n",
    "     *    \n",
    "\\left( \\begin{array}{c}\n",
    "v_{0} \\\\ \n",
    "v_{1} \\\\ \n",
    "v_{2} \\\\ \n",
    "v_{3}\n",
    "\\end{array} \\right)\n",
    "     =\n",
    "\\left( \\begin{array}{c}\n",
    "a_{00}*v_{0} + a_{01}*v_{1} + a_{02}*v_{2} + a_{03}*v_{3}  \\\\ \n",
    "a_{10}*v_{0} + a_{11}*v_{1} + a_{12}*v_{2} + a_{13}*v_{3} \\\\ \n",
    "a_{20}*v_{0} + a_{21}*v_{1} + a_{22}*v_{2} + a_{23}*v_{3} \\\\ \n",
    "a_{30}*v_{0} + a_{31}*v_{1} + a_{32}*v_{2} + a_{33}*v_{3}\n",
    "\\end{array} \\right)\n",
    "    $$\n",
    "    \n",
    "Die Voraussetzung das zwei Matritzen *A* , *v* miteinander multipliziert werden können ist das die Anzahl der spalten in *A* der Anzahl der Zeilen in *v* entspricht. Da in diesem Fall *v* ein Vektor ist, muss er genau so viele Dimensionen haben wie die Matrix Spalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Was versteht man im Kontext der NNMF unter\n",
    "    * Merkmalsmatrix\n",
    "    * Gewichtsmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wie werden in Numpy zwei Arrays (Typ numpy.array) \n",
    "\t* im Sinne der Matrixmultiplikation miteinander multipliziert?\n",
    "\t* elementweise multipliziert?\n",
    "* Wie wird die Transponierte eines Numpy-Arrays berechnet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Elementweise:**\n",
    "\n",
    "`np.multiply(x1, x2)`\n",
    "\n",
    "**Matrixmultiplikation:**\n",
    "\n",
    "`np.matmul(a, b)`\n",
    "\n",
    "**Transponierten:**\n",
    "\n",
    "`np.transpose(x)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versuchsdurchführung\n",
    "Die in diesem Versuch einzubindenden Feeds sind in der unten stehenden Liste _feedlist_ definiert. Die aus dem vorigen Vesuch bereits bekannte Funktion _stripHTML()_ ist ebenfalls gegeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "feedlist=['http://feeds.reuters.com/reuters/topNews',\n",
    "          'http://feeds.reuters.com/reuters/businessNews',\n",
    "          'http://feeds.reuters.com/reuters/worldNews',\n",
    "          'http://feeds2.feedburner.com/time/world',\n",
    "          'http://feeds2.feedburner.com/time/business',\n",
    "          'http://feeds2.feedburner.com/time/politics',\n",
    "          'http://rss.cnn.com/rss/edition.rss',\n",
    "          'http://rss.cnn.com/rss/edition_world.rss',\n",
    "          'http://www.nytimes.com/services/xml/rss/nyt/GlobalHome.xml',\n",
    "          'http://feeds.nytimes.com/nyt/rss/Business',\n",
    "          'http://www.nytimes.com/services/xml/rss/nyt/World.xml',\n",
    "          'http://www.nytimes.com/services/xml/rss/nyt/Economy.xml'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stripHTML(h):\n",
    "  p=''\n",
    "  s=0\n",
    "  for c in h:\n",
    "    if c=='<': s=1\n",
    "    elif c=='>':\n",
    "      s=0\n",
    "      p+=' '\n",
    "    elif s==0: p+=c\n",
    "  return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Articles------------------------\n",
      "******************************\n",
      "http://feeds.reuters.com/reuters/topNews\n",
      "******************************\n",
      "http://feeds.reuters.com/reuters/businessNews\n",
      "******************************\n",
      "http://feeds.reuters.com/reuters/worldNews\n",
      "******************************\n",
      "http://feeds2.feedburner.com/time/world\n",
      "******************************\n",
      "http://feeds2.feedburner.com/time/business\n",
      "******************************\n",
      "http://feeds2.feedburner.com/time/politics\n",
      "******************************\n",
      "http://rss.cnn.com/rss/edition.rss\n",
      "******************************\n",
      "http://rss.cnn.com/rss/edition_world.rss\n",
      "******************************\n",
      "http://www.nytimes.com/services/xml/rss/nyt/GlobalHome.xml\n",
      "******************************\n",
      "http://feeds.nytimes.com/nyt/rss/Business\n",
      "******************************\n",
      "http://www.nytimes.com/services/xml/rss/nyt/World.xml\n",
      "******************************\n",
      "http://www.nytimes.com/services/xml/rss/nyt/Economy.xml\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rssfeeds = pd.DataFrame(columns=['title','text','source'])\n",
    "\n",
    "print \"--------------------Articles------------------------\"\n",
    "for feed in feedlist:\n",
    "    print \"*\"*30\n",
    "    print feed\n",
    "    f=feedparser.parse(feed)\n",
    "    for e in f.entries:\n",
    "      #print '\\n---------------------------'\n",
    "      if 'summary' in e.keys() and 'title' in e.keys():\n",
    "        text = stripHTML(e.description)\n",
    "        title = stripHTML(e.title)\n",
    "        rssfeeds = rssfeeds.append(pd.DataFrame({'title': title, 'text': text, 'source': feed}, index=[0]), ignore_index=True)\n",
    "print \"------------------------------------------------------\"\n",
    "print \"------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                source  \\\n",
      "124           http://rss.cnn.com/rss/edition_world.rss   \n",
      "163  http://www.nytimes.com/services/xml/rss/nyt/Gl...   \n",
      "36             http://feeds2.feedburner.com/time/world   \n",
      "30             http://feeds2.feedburner.com/time/world   \n",
      "101           http://rss.cnn.com/rss/edition_world.rss   \n",
      "\n",
      "                                                  text  \\\n",
      "124  Hooded men broke into a shop in Paris and atte...   \n",
      "163  Attendees worked side by side during a Quilt c...   \n",
      "36   Previously, female fans could only watch sport...   \n",
      "30   Eight suspected members of an online pedophile...   \n",
      "101  President Donald Trump is about to come face-t...   \n",
      "\n",
      "                                                 title  \n",
      "124                                                     \n",
      "163  Come on Over to My Place, Sister Girlfriend, a...  \n",
      "36   Women in Saudi Arabia Can Finally Attend Live ...  \n",
      "30   Mother Let Others Rape Her Son for Money as Pa...  \n",
      "101  Opinion: Scrapping Iran deal will only harm Am...  \n"
     ]
    }
   ],
   "source": [
    "print rssfeeds.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anlegen der Artikel/Wort-Matrix\n",
    "\n",
    "### Die Funktion _getarticlewords()_\n",
    "Schreiben Sie eine Funktion _getarticlewords()_, die folgende Elemente zurückgibt:\n",
    "\n",
    "* _allwords:_ ist ein Dictionary dessen Keys die Worte aller gesammelten Artikel sind. Der zu jedem Key gehörende Wert ist die Anzahl, wie oft das Wort insgesamt vorkommt.\n",
    "* _articlewords:_ ist eine Liste mit so vielen Elementen wie Artikel in der Sammlung sind. Jedes Listenelement ist ein Dictionary, welches die Worte des jeweiligen Artikels als Key enthält und als Wert die Worthäufigkeit.\n",
    "* _articletitles_ ist eine Liste mit so vielen Elementen wie Artikel in der Sammlung sind. Jedes Element ist der Artikeltitel als String.\n",
    "\n",
    "Für das Parsing der Feeds soll wieder das Modul _feedparser_ eingesetzt werden. Die zu einer Nachricht gehörenden Wörter sollen die Wörter des Elements _title_ und die Wörter des Elements _description_ sein (siehe voriger Versuch). Allerdings sollen hier nicht alle Wörter eingebunden werden, sondern wie im vorigen Versuch eine Methode _getwords()_ implementiert werden, welche nur die _relevanten_ Wörter zurückgibt. Die Frage welche Wörter relevant sind ist nicht eindeutig beantwortbar. Sie können sich hierzu eigene Antworten einfallen lassen. Auf jeden Fall sollten aber die Stopwörter ignoriert werden. Hierzu kann z.B. die Stopwortliste von NLTK angewandt werden.\n",
    "\n",
    "Nachdem alle relevanten Wörter aller Nachrichten gesammelt sind, sollte eine weitere Bereinigung stattfinden, die \n",
    "\n",
    "* alle Wörter, die weniger als 4 mal vorkommen\n",
    "* alle Wörter, die in mehr als 30% aller Dokumente vorkommen\n",
    "\n",
    "entfernt. \n",
    "\n",
    "Durch dieses Herausfiltern nicht relevanter Wörter kann es vorkommen, dass einzelne Artikel keine relevanten Wörter mehr enthalten. Diese Artikel sollen dann ganz ignoriert werden. D.h. unter anderem, dass diese Artikel auch nicht in _articlewords_ und _articletitles_ erscheinen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marius/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "491\n",
      "277\n",
      "{u'trump': 2, u'washington': 1, u'u s': 1, u'haiti african': 2, u'donald': 1, u'immigr': 1, u'presid donald': 1, u'home': 1, u'face': 1, u'unit': 1, u'condemn': 2, u'african countri': 2, u'use': 1, u'said': 1, u'describ': 1, u'would': 1, u'reuter': 1, u'bad': 1, u'state': 1, u'senat': 1, u'washington reuter': 1, u'forc': 1, u'peopl': 1, u'friday': 1, u'presid': 1, u'donald trump': 1, u'plan': 1, u'haiti': 2, u'abroad': 1, u'trump friday': 1, u'remark': 1, u'word': 1, u'critic': 1, u'unit state': 1, u's': 1, u'deni': 2, u'u': 1, u'african': 2, u'countri': 3, u'shithol': 1}\n",
      "\n",
      "{u'last month': 5, u'founder': 5, u'global': 7, u'month': 9, u'protest': 4, u'go': 4, u'follow': 8, u'children': 5, u'thursday': 8, u'trump excel': 4, u'minist': 5, u'deal presid': 4, u'facebook': 22, u'govern': 7, u'administr': 4, u'die': 4, u'hous': 14, u'wednesday': 9, u'chang news': 4, u'race': 5, u'saudi': 6, u'team': 10, u'marri': 4, u'trump friday': 4, u'vehicl': 4, u'sign': 6, u'street': 5, u'video': 4, u'n': 4, u'even': 4, u'what': 13, u'repriev': 4, u'near': 4, u'suspect': 4, u'new': 46, u'appeal': 4, u'public': 5, u'disney': 4, u'friday': 31, u'iran': 21, u'men': 6, u'chief execut': 8, u'news feed': 10, u'bodi': 4, u'china': 13, u'100': 6, u'sinc': 5, u'remark': 10, u'militari': 5, u'prime minist': 5, u'technolog': 4, u'credit': 5, u'chrysler': 7, u'social': 8, u'action': 4, u'chang': 16, u'honor': 4, u'king': 4, u'depart': 4, u'africa': 4, u'ask': 5, u'famili': 9, u'sanction': 7, u'market': 6, u'saudi arabia': 4, u'from': 14, u'describ': 5, u'would': 16, u'visit': 4, u'two': 11, u'next': 4, u'websit': 5, u'live': 10, u'call': 14, u'6': 5, u'australian': 5, u'peopl': 12, u'hundr': 4, u'warn': 7, u'compani': 19, u'women': 12, u'shoot': 5, u'word': 4, u'f': 5, u'this': 6, u'car': 10, u'work': 10, u'histori': 5, u'vega': 5, u'can': 4, u'mr': 7, u'meet': 7, u'control': 4, u'claim': 7, u'iran nuclear': 7, u'trump': 92, u'give': 8, u'share': 4, u'high': 5, u'arabia': 4, u'critic': 5, u'want': 7, u'onlin': 4, u'recycl': 4, u'agenc': 6, u'excel health': 5, u'condemn': 4, u'surfac': 4, u'end': 4, u'secur': 7, u'polic': 4, u'reuter': 31, u'reuter the': 4, u'ceremoni': 4, u'how': 8, u'anyon': 4, u'washington reuter': 9, u'stock': 5, u'plant': 6, u'attend': 5, u'may': 8, u'chines': 8, u'after': 5, u'court': 6, u'law': 4, u'grow': 6, u'physic': 4, u'las': 5, u'a': 41, u'short': 4, u'attempt': 5, u'said friday': 4, u'issu': 5, u'chief': 12, u'african': 11, u'fan': 5, u'feed': 13, u'embassi': 6, u'help': 6, u'offici': 8, u'move': 8, u'trade': 5, u'london': 11, u'still': 5, u'dreamer': 5, u'1': 11, u'overhaul': 4, u'polici': 6, u'fix': 9, u'offic': 5, u'2018': 6, u'2017': 6, u'2016': 5, u'senat': 5, u'2010': 4, u'good': 7, u'recal': 5, u'financi': 4, u'beij': 6, u'nation': 10, u'not': 4, u'now': 4, u'investor': 5, u'discuss': 5, u'bank': 6, u'term': 5, u'year old': 8, u'l': 4, u'unit state': 6, u'found': 6, u'mean': 4, u'republican': 4, u'russia deploy': 4, u'ed': 7, u'surfac air': 4, u'expect': 7, u'year': 27, u'girl': 7, u'sexual': 5, u'saturday': 12, u'out': 5, u'health': 8, u'wide': 4, u'calif': 5, u'factori': 4, u'insid': 4, u'million': 9, u'california': 7, u'york': 6, u'put': 5, u'fiat': 5, u'assang': 4, u'could': 11, u'recov': 5, u'american': 9, u'walmart': 5, u'iphon': 4, u'syria': 7, u'retail': 5, u'south': 16, u'first': 14, u'oper': 5, u'make chang': 4, u'carri': 4, u'u s': 19, u'one': 17, u'agre': 5, u'open': 13, u'size': 4, u'interact': 4, u'bitcoin': 4, u'least': 7, u'plastic': 4, u'citi': 7, u'attack': 6, u'new york': 6, u'white': 14, u'john': 5, u'final': 4, u'friend': 8, u'includ': 8, u'op ed': 7, u'relationship': 5, u'provinc': 4, u'hotel': 5, u'took': 4, u'part': 7, u'1 000': 4, u'11': 6, u'10': 4, u'14': 4, u'iranian': 5, u'stall': 4, u'store': 5, u'shithol': 10, u'russia': 5, u'salvag': 4, u'say': 27, u'immigr': 8, u'presid donald': 18, u'sell': 5, u'caus': 4, u'take': 7, u'begin': 4, u'germani': 4, u'deploy': 4, u'minivan': 4, u'sue': 4, u'paid': 5, u'presid': 46, u'plan': 8, u'letter': 7, u'america': 4, u'charg': 5, u'iran deal': 5, u'sale': 6, u'face': 11, u'deni': 6, u'left': 8, u'show': 11, u'queen': 4, u'air missil': 4, u'find': 5, u'execut': 10, u'3': 4, u'hockey': 4, u'8': 4, u'over': 10, u'his': 6, u'hit': 4, u'get': 4, u'merkel': 4, u'watch': 4, u'amazon': 4, u'outlet': 4, u'report': 17, u'haiti': 14, u'target': 5, u'prime': 5, u'earn': 4, u'white hous': 10, u'bad': 5, u'she': 9, u'ban': 6, u'tanker': 4, u'wrote': 4, u'view': 4, u'art': 6, u'see': 7, u'are': 5, u'sea': 5, u'respons': 6, u'close': 5, u'news': 25, u'sport': 4, u'said': 24, u'nuclear': 17, u'scrap': 5, u'state': 22, u'import': 5, u'across': 5, u'mass shoot': 4, u'we': 6, u'korea': 18, u'exam': 5, u'extend': 4, u'jr': 4, u'group': 7, u'lawsuit': 4, u'last': 14, u'quiet': 4, u'foreign': 5, u's': 22, u'columnist': 4, u'mani': 6, u'comment': 8, u'co': 4, u'cancer': 4, u'jeff': 8, u'wall': 6, u'haiti african': 5, u'fiat chrysler': 5, u'speak': 5, u'west': 6, u'three': 7, u'secret': 7, u'a unifi': 4, u'life': 9, u'gay': 4, u'fire': 11, u'worker': 5, u'search': 4, u'fund': 8, u'former': 8, u'east': 4, u'look': 7, u'servic': 4, u'britain': 6, u'air': 6, u'bezo': 8, u'unifi': 4, u'gave': 4, u'disparag': 4, u'ed columnist': 4, u'shithol countri': 5, u'is': 19, u'it': 15, u'sudden': 4, u'outrag': 4, u'return': 4, u'news agenc': 5, u'develop': 4, u'media': 8, u'make': 22, u'member': 6, u'come': 8, u'document': 5, u'european': 5, u'week': 11, u'oil': 7, u'donald trump': 25, u'000': 15, u'countri': 24, u'chines salvag': 4, u'recov two': 4, u'center': 8, u'i': 10, u'indonesia': 4, u'person': 4, u'crimean': 4, u'the': 66, u'scholarship': 6, u'ambassador': 4, u'presid trump': 19, u'just': 4, u'money': 4, u'kill': 7, u'human': 4, u'facebook make': 4, u'cut': 9, u'day': 12, u'arrest': 6, u'book': 7, u'pile': 4, u'4': 4, u'tax': 7, u'has': 4, u'warrant': 4, u'match': 5, u'build': 9, u'big': 5, u'five': 4, u'know': 4, u'world': 18, u'like': 10, u'loss': 4, u'd': 9, u'toyota': 4, u'benefit': 4, u'journalist': 4, u'martin': 4, u'manag': 4, u'nuclear deal': 10, u'soccer': 7, u'right': 4, u'old': 9, u'trump shithol': 4, u'deal': 21, u'back': 9, u'donald': 25, u'championship': 4, u'home': 10, u'reuter a': 4, u'feder': 5, u'for': 4, u'legal': 5, u'unit': 8, u'cnn': 7, u'leader': 5, u'be': 5, u'who': 4, u'run': 6, u'power': 5, u'us presid': 5, u'use': 7, u'whi': 7, u'step': 6, u'corp': 4, u'investig': 4, u'post': 4, u'abroad': 4, u'about': 7, u'troubl': 4, u'interact friend': 4, u'marriott': 4, u'op': 7, u'trump administr': 4, u'gay australian': 4, u'mudslid': 9, u'washington': 10, u'announc': 5, u'your': 8, u'trump issu': 4, u'african countri': 6, u'her': 4, u'support': 4, u'fast': 4, u'fight': 5, u'start': 5, u'appl': 7, u'low': 7, u'south korea': 13, u'was': 7, u'war': 5, u'buy': 4, u'forc': 9, u'brand': 5, u'las vega': 5, u'but': 5, u'russian': 4, u'with': 9, u'he': 12, u'made': 12, u'up': 5, u'us': 16, u'record': 4, u'jeff bezo': 8, u'uber': 8, u'an': 8, u'as': 4, u'at': 5, u'two bodi': 4, u'inc': 10, u'excel': 5, u'other': 8, u'mass': 5, u'you': 5, u'repeat': 4, u'star': 6, u'congress': 4, u'missil': 5, u'friend famili': 4, u'billion': 10, u'e': 9, u'algorithm': 4, u'wife': 4, u'invest': 6, u'rule': 5, u'u': 24, u'potenti': 4, u'time': 8}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import ngrams\n",
    "\n",
    "def to_dict(d):\n",
    "    '''helper class – converts defaultdict to regular dict'''\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {k: to_dict(v) for k, v in d.iteritems()}\n",
    "    return d\n",
    "\n",
    "def getwords_adv(doc):\n",
    "    \n",
    "    #doc = doc.lower()\n",
    "    \n",
    "    #tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    \n",
    "    #remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #stemming\n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    #add bigrams\n",
    "    docsting = ' '.join(tokens).strip()\n",
    "    n = 2\n",
    "    n_grams = ngrams(docsting.split(), n)\n",
    "    tokens = tokens + [' '.join(grams) for grams in n_grams]\n",
    "    \n",
    "    #remove words smaller than 4 and bigger than 60\n",
    "    #tokens = [token for token in tokens if len(token) in range(4,60)]\n",
    "    return tokens\n",
    "\n",
    "def getarticlewords(articles):\n",
    "    allwords = defaultdict(int)\n",
    "    articletitles = []\n",
    "    articlewords = []\n",
    "    \n",
    "    for index, row in articles.iterrows():\n",
    "        title = row.loc['title']\n",
    "        fulltext = row.loc['title'] + ' ' + row.loc['text']\n",
    "        # add articletitles\n",
    "        articletitles.append(title)\n",
    "        # add articlewords\n",
    "        words = getwords_adv(fulltext)\n",
    "        wordsdict = defaultdict(int)\n",
    "        for word in words: \n",
    "            wordsdict[word] += 1\n",
    "            # extend allwords\n",
    "            allwords[word] += 1\n",
    "        articlewords.append(wordsdict)\n",
    "    \n",
    "    #remove words where count is lower 4\n",
    "    wordsToRemove = []\n",
    "    for word,count in allwords.items():\n",
    "        if count < 4:\n",
    "            #gather words to be removed\n",
    "            wordsToRemove.append(word)\n",
    "            #remove from allwords\n",
    "            allwords.pop(word, None)\n",
    "    \n",
    "    #remove words from articlewords\n",
    "    for words in articlewords:\n",
    "        for word in wordsToRemove:\n",
    "            words.pop(word, None)\n",
    "    #remove word that are included in more than 30% of all documents\n",
    "    \n",
    "    return to_dict(allwords),articlewords,articletitles\n",
    "\n",
    "allwords,articlewords,articletitles = getarticlewords(rssfeeds)\n",
    "print len(allwords)\n",
    "print len(articlewords)\n",
    "print to_dict(articlewords[0])\n",
    "print '\\n',to_dict(allwords)\n",
    "#print to_dict([to_dict(words) for words in articlewords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Funktion _makematrix()_\n",
    "Schreiben Sie eine Funktion _makematrix()_, die aus dem Dictionary _allwords_ und der Liste _articlewords_ (vorige Aufgabe) die Artikel-/Wort-Matrix generiert. Die Einträge in der Matrix sollen die Häufigkeiten der Wörter im jeweiligen Dokument sein (term frequency tf). Die Artikel-/Wort-Matrix soll als 2-dimensionales Numpy Array angelegt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(277, 491)\n",
      "[[0 0 0 ..., 1 0 0]\n",
      " [0 0 0 ..., 1 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "'''def makematrix(allwords, articlewords):\n",
    "    \n",
    "    dictionary = corpora.Dictionary(articlewords)\n",
    "    v = dictionary.token2id\n",
    "    print v\n",
    "    #A = \n",
    "    print(\"Total number of documents in the dictionary: \",dictionary.num_docs)\n",
    "    print(\"Total number of corpus positions: \",dictionary.num_pos)\n",
    "    print(\"Total number of non-zeros in the BoW-Matrix: \",dictionary.num_nnz)\n",
    "    print(\"Total number of different words in the dictionary: \",len(dictionary))'''\n",
    "    \n",
    "# makematrix(allwords, articlewords)\n",
    "\n",
    "def get_row(all_words, article_words):\n",
    "    for word, value in all_words.items():\n",
    "        try:\n",
    "            all_words[word] = article_words[word]\n",
    "        except KeyError:\n",
    "            value = 0\n",
    "    article_array = np.array(all_words.values()) \n",
    "    return article_array\n",
    "\n",
    "def makematrix(article_words, all_words):\n",
    "    A = np.empty(shape=(0,len(all_words)), dtype=int) #<- hier stimmt was noch net\n",
    "    for words_of_single_article in article_words:\n",
    "        v = get_row(all_words, words_of_single_article)\n",
    "        A = np.vstack((A, v))\n",
    "    return A\n",
    "\n",
    "matrix_A = makematrix(articlewords,allwords)\n",
    "print matrix_A.shape\n",
    "print matrix_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Nicht Negative Matrix Faktorisierung\n",
    "Die Implementierung der NNMF ist entsprechend der Beschreibung im Theoriekapitel durchzuführen.\n",
    "\n",
    "* Implementieren Sie die Funktion _cost(A,B)_. Dieser Funktion werden zwei Numpy-Matrizen $A$ und $B$ übergeben. Zurück geliefert werden die nach oben angegebener Formel berechneten Kosten $k$. Diese Funktion wird von der im folgenden beschriebenen Funktion _nnmf(A,m,it)_ benutzt.\n",
    "* Implementieren Sie die Funktion __nnmf(A,m,it)__. In dieser Funktion soll der oben beschriebene Algorithmus für die Nicht-negative Matrix Faktorisierung ausgeführt werden. Der Funktion wird die zu faktorisierende Matrix $A$, die Anzahl der Merkmale $m$ und die Anzahl der Iterationen $it$ übergeben. Die Funktion gibt die gefundenen Faktoren $W$ und $H$ zurück. In jeder Iteration sollen mit der Funktion __cost(A,B)__ die Kosten berechnet werden. Sobald die Kostenabnahme pro 10 Iterationen kleiner als $2$ ist oder eine maximale Anzahl von Iterationen ($maxIt=200$) erreicht ist, soll der Algorithmus mit der Rückgabe der Faktoren $W$ und $H$ terminieren.     \n",
    "\n",
    "\n",
    "Tipp für die Implementierung elementweiser Operationen von Matrizen: Für elementweise Operationen müssen in Python/Numpy nicht alle Elemente über Schleifen explizit berechnet werden. Eine elementweise Anpassung aller Matrixelemente kann kompakt programmiert werden indem die beteiligten Matrizen für diese Operationen als Arrays implementiert werden. Sollen z.B. die beiden gleich großen Numpy Arrays $U$ und $V$ elementweise multipliziert werden, dann wäre der entsprechende Programmcode einfach _U*V_.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 [4604.3327277225999, 4595.014293419491, 4587.0674655182229, 4580.3082138982209, 4574.5825483815561, 4569.7458479466432, 4565.6637030820175, 4562.2156016511344, 4559.2971235116302, 4556.8198924377475, 4554.7100739818752, 4552.9063394599361, 4551.3578827385336, 4550.022723955668, 4548.8662975250818, 4547.8602425380732, 4546.9813396735944, 4546.2105802421738, 4545.5323671801461, 4544.9338434459432, 4544.4043375162037, 4543.9349138769503, 4543.5180164078156, 4543.1471920078648, 4542.8168806360409, 4542.522257384333, 4542.2591131540848, 4542.0237628012519, 4541.8129724827704, 4541.6239005378193, 4541.4540481820659, 4541.3012175553431, 4541.1634754420811] \n",
      "\n",
      "\n",
      "[[  8.21093688e-03   7.37982334e-07   1.24872240e-02 ...,   1.07095681e-05\n",
      "    2.80634302e-02   1.47722961e-03]\n",
      " [  6.63294915e-11   7.45894194e-06   2.96207605e-03 ...,   3.74185197e-05\n",
      "    1.04042584e-04   1.96852276e-06]\n",
      " [  2.23113305e-05   3.22215654e-02   7.94366458e-03 ...,   7.11853012e-02\n",
      "    1.90826421e-12   4.00180254e-04]\n",
      " ..., \n",
      " [  1.10120470e-16   2.15762766e-05   1.03889322e-03 ...,   7.32225855e-04\n",
      "    9.01422415e-05   1.38204136e-02]\n",
      " [  3.51556680e-02   1.09094790e-08   2.07286126e-11 ...,   3.96708121e-03\n",
      "    1.26068920e-05   4.13928706e-10]\n",
      " [  3.56962653e-05   3.13829820e-07   1.93820432e-02 ...,   4.31353375e-05\n",
      "    5.91299633e-05   4.23723042e-06]] [[  6.91263877e-01   1.22343157e+00   4.35474725e+00 ...,   3.82715674e+00\n",
      "    7.82271847e-01   1.20752486e+00]\n",
      " [  6.12179420e-03   6.77930248e-01   1.00289328e+00 ...,   2.97213571e-02\n",
      "    4.54655470e-01   2.58341869e-01]\n",
      " [  4.28075689e-01   1.66963765e+00   1.17925910e+00 ...,   3.74994784e-01\n",
      "    2.15221674e-02   5.60194744e-01]\n",
      " ..., \n",
      " [  1.01891203e+00   1.77266795e-02   1.12275248e+00 ...,   4.51076898e-02\n",
      "    8.78702904e-03   1.20851867e-04]\n",
      " [  9.54804329e-01   4.43670474e-01   2.45301698e-01 ...,   5.51463647e-01\n",
      "    2.59417799e-01   3.50653181e-01]\n",
      " [  1.39653659e+00   7.24870323e-02   5.90448682e-02 ...,   5.35874013e-01\n",
      "    1.64265475e-01   9.82315446e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Erstellen der Matrix H und W\n",
    "def generate_random_matrix(rows, columns):\n",
    "    return np.random.random((rows, columns))\n",
    "\n",
    "count_of_features = 15 # (m) Merkmale\n",
    "row_count_matrix_A = len(matrix_A)\n",
    "#print row_count_matrix_A\n",
    "col_count_matrix_A = len(matrix_A[2])\n",
    "\n",
    "matrix_H = generate_random_matrix(count_of_features,col_count_matrix_A)\n",
    "matrix_W = generate_random_matrix(row_count_matrix_A, count_of_features)\n",
    "\n",
    "# Erstellen der Matrix B, mit B = W * H\n",
    "\n",
    "matrix_B = np.matmul(matrix_W, matrix_H)\n",
    "\n",
    "#print matrix_B\n",
    "\n",
    "# Berechnung Kostenfaktor k\n",
    "\n",
    "def calc_cost(matrix_A, matrix_B):\n",
    "    sub_matrix = np.subtract(matrix_A, matrix_B)\n",
    "    sum_of_all = 0\n",
    "    for v in sub_matrix:\n",
    "        for v_2 in v:\n",
    "            sum_of_all += v_2*v_2\n",
    "    return sum_of_all\n",
    "\n",
    "#print calc_cost(matrix_A, matrix_B)\n",
    "\n",
    "def adapt_H(A, H, W):\n",
    "    W_t = np.transpose(W)\n",
    "    nummerator = np.matmul(W_t,A)\n",
    "    denominator = np.matmul(np.matmul(W_t,W), H)\n",
    "    fract = np.divide(nummerator,denominator)\n",
    "    result = H*fract # fraglich\n",
    "    return result\n",
    "    \n",
    "def adapt_W(A, H, W):\n",
    "    H_t = np.transpose(H)\n",
    "    nummerator = np.matmul(A, H_t)\n",
    "    denominator = np.matmul(np.matmul(W,H), H_t)\n",
    "    fract = np.divide(nummerator,denominator)\n",
    "    result = W*fract # fraglich\n",
    "    return result\n",
    "    \n",
    "def nnmf(A,m,it):\n",
    "    H = generate_random_matrix(count_of_features,col_count_matrix_A)\n",
    "    W = generate_random_matrix(row_count_matrix_A, count_of_features)    \n",
    "    cost_list = []\n",
    "    for i in range(it):\n",
    "        H = adapt_H(A, H, W)\n",
    "        B = np.matmul(W, H)\n",
    "        cost_list.append(calc_cost(A, B))\n",
    "        if len(cost_list) > 10 and (cost_list[-10] - cost_list[-1]) < 2.0:\n",
    "            break\n",
    "    W = adapt_W(A, H, W)\n",
    "    return (i, cost_list,H,W)\n",
    "\n",
    "interations, costs, matrix_H, matrix_W = nnmf(matrix_A, 30, 200)\n",
    "\n",
    "print interations, costs, '\\n','\\n','\\n',  matrix_H, matrix_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anzeige der Merkmale und der Gewichte\n",
    "\n",
    "Im vorigen Abschnitt wurde die Merkmalsmatrix $H$ und die Gewichtsmatrix $W$ berechnet. Diese Matrizen können natürlich am Bildschirm ausgegeben werden, was jedoch nicht besonders informativ ist. Aus den Matrizen können jedoch die Antworten für die folgenden interessanten Fragen berechnet werden:\n",
    "\n",
    "* In welchen Artikeln sind welche Merkmale stark vertreten?\n",
    "* Wie lassen sich die insgesamt $m$ Merkmale beschreiben, so dass aus dieser Merkmalsbeschreibung klar wird, welches Thema den Artikeln, in denen das Merkmal stark vertreten ist, behandelt wird? \n",
    " \n",
    "Die Antwort auf die erste Frage ergibt sich aus der Gewichtsmatrix $W$. Für die Beantwortung der zweiten Frage wird die Merkmalsmatrix $H$ herangezogen.\n",
    "\n",
    "\n",
    "\n",
    "### Beschreibung der Merkmale\n",
    "\n",
    "Die Merkmalsmatrix $H$ beschreibt, wie stark die Worte aus _wordvec_ in jedem Merkmal enthalten sind. Jede Zeile von $H$ gehört zu einem Merkmal, jede Spalte von $H$ gehört zu einem Wort in _wordvec_.\n",
    "\n",
    "Es bietet sich an jedes Merkmal einfach durch die $N=6$ Wörter aus _wordvec_ zu beschreiben, welche am stärksten in diesem Merkmal enthalten sind. Hierzu muss für jedes Merkmal die entsprechende Zeile in $H$ nach den $N=6$ größten Werten durchsucht bzw. geordnet werden. Die entsprechenden Spalten dieser Matrixelemente verweisen dann auf die $N=6$ wichtigsten Worte des Merkmals.\n",
    "\n",
    "Tipp für die Implementierung: Legen Sie für jedes Merkmal $i$ eine Liste an. Die Listenlänge ist durch die Anzahl der Worte in _wordvec_ (d.h. die Anzahl der Spalten in $H$) gegeben. Jedes Listenelement $j$ enthält selbst wieder 2 Elemente: An erster Stelle den entsprechenden Wert $H_{i,j}$ der Merkmalsmatrix, an der zweiten Stelle das $j.$-te Wort in _wordvec_. Nachdem die Liste angelegt ist, kann sie mit _listname.sort()_ in aufsteigender Reihenfolge sortiert werden. Die abnehmende Sortierung erhält man mit _listname.sort().reverse()_. Danach geben die $N=6$ ersten Listenelemente die für das Merkmal $i$ wichtigsten Worte an.\n",
    "\n",
    "   \n",
    "### Präsenz der Merkmale in den Artikeln\n",
    "\n",
    "Die Gewichtsmatrix $W$ beschreibt, wie stark die $m$ Merkmale in den Artikeln aus _articletitles_ enthalten sind. Jede Zeile von $W$ gehört zu einem Artikel, jede Spalte von $W$ gehört zu einem Merkmal.\n",
    "Die Berechnung der $M=2$ gewichtigsten Merkmale für jeden Artikel in _articletitles_ kann analog zu der oben beschriebenen Berechnung der $N=6$ wichtigsten Worte eines Merkmals berechnet werden.\n",
    "\n",
    "\n",
    "### Implementierung\n",
    "\n",
    "Implementieren Sie eine Funktion _showfeatures(w,h,titles,wordvec)_, welche wie oben beschrieben für jeden Artikel die $M=2$ wichtigsten Merkmale am Bildschirm ausgibt. Dabei soll jedes Merkmal durch die 6 wichtigsten Wörter dieses Merkmals angegeben werden. Siehe Beispielausgabe unten.  \n",
    "\n",
    "Übergabeparameter der Funktion sind die Merkmalsmatrix $H$, die Gewichtungsmatrix $W$, die Liste aller Artikeltitel _articletitles_ und die Liste aller Worte _wordvec_.\n",
    "\n",
    "\n",
    "Beispiel fuer Ausgabe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(13.54131155883748, 13, u'Putin vows payback after confirmation of Egypt plane bomb'),\n",
    "\n",
    "(2.2466669548146254, 9, u'Putin vows payback after confirmation of Egypt plane bomb')]\n",
    "\n",
    "----- ['plane', 'egypt', 'russia', 'month', 'killing', 'putin']\n",
    "\n",
    "----- ['airport', 'russian', 'crash', 'egypt', 'security', 'officials']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe ist wie folgt zu interpretieren:\n",
    "* Für den Artikel _Putin vows payback after confirmation of Egypt plane bomb_ ist \n",
    "    * das wichtigste Merkmal durch die 6 Wörter _plane_, _egypt_, _russia_, _month_, _killing_, _putin_ definiert. Das Gewicht dieses Merkmals im Artikel ist 13.54\n",
    "    * das zweitwichtigste Merkmal durch die 6 Wörter _airport_, _russian_, _crash_, _egypt_, _security_, _officials_ definiert. Das Gewicht dieses Merkmals im Artikel ist 2.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(6, 0.055627528043210549), (42, 0.059762686749649144), (136, 0.062927918657517207), (30, 0.064483897448640234), (34, 0.079518751089576553), (353, 0.089829012488288201)], [(205, 0.034787080603170088), (199, 0.036991366544601605), (393, 0.052741678063339292), (237, 0.053192518063817848), (128, 0.075994579023961797), (303, 0.076455664313787614)], [(16, 0.051764254696896599), (293, 0.054822368406976719), (307, 0.056374429337523145), (298, 0.057746242236197808), (488, 0.071185301234281087), (34, 0.080968139805558489)], [(472, 0.053859531196461134), (488, 0.0560569102070946), (38, 0.066371733508431466), (204, 0.067529442579129129), (12, 0.068193154205559714), (79, 0.086817363750032003)], [(130, 0.023818270388038271), (40, 0.024268370900184411), (268, 0.028904594031264455), (90, 0.031076918037640778), (431, 0.038056555647777539), (34, 0.084178960264213537)], [(317, 0.037135683541428217), (353, 0.047679193749597358), (142, 0.047943596439308636), (370, 0.051833862535535333), (111, 0.053444784035751258), (55, 0.063894142689789754)], [(183, 0.031305503169339569), (251, 0.031856116390223101), (45, 0.042181782477153845), (394, 0.045724487185810667), (380, 0.045778049493190802), (295, 0.050720799916900469)], [(405, 0.060981024724899857), (276, 0.062678020436327309), (362, 0.063182451952786531), (373, 0.091077885899643843), (128, 0.10856225764247562), (249, 0.11024487172962012)], [(119, 0.037842982712985029), (388, 0.037936852214618386), (194, 0.038129581934926979), (138, 0.03832132685661456), (461, 0.042835340897680971), (38, 0.049831655250136997)], [(207, 0.045411915010207242), (158, 0.046877673701480285), (237, 0.050634082199443396), (187, 0.052760062319503025), (192, 0.062314562823641811), (62, 0.063145892806356643)], [(464, 0.059519064513543274), (249, 0.06543698487665528), (407, 0.066550275203654016), (360, 0.071465593235369843), (409, 0.080490847270426241), (361, 0.096530434757546357)], [(476, 0.036659119759793753), (3, 0.037435963016028855), (475, 0.039060130843223663), (275, 0.047204434278215607), (464, 0.049008415970702414), (346, 0.05133228085413466)], [(69, 0.050605222482709877), (133, 0.051106318171646757), (296, 0.052708187108602637), (200, 0.052726435689947358), (393, 0.059279667708796439), (263, 0.063398472944093676)], [(39, 0.048640738902666), (186, 0.04897081433612499), (373, 0.051742280466271551), (407, 0.059765946865868225), (345, 0.065038948714850428), (249, 0.098597991657836948)], [(63, 0.041308509823391869), (362, 0.048507878174458126), (303, 0.052385598474817573), (358, 0.053550831583509137), (73, 0.058073636579982546), (199, 0.059513623134367914)]]\n",
      "[[(12, 3.8271567355226672), (2, 4.3547472461894623)], [(6, 2.299151731846659), (5, 4.5473503795162298)], [(1, 1.6696376462709777), (4, 3.0519687494341068)], [(4, 2.7928038128585815), (12, 4.3542207797458001)], [(8, 0.79953047336786787), (14, 0.94219632973113088)], [(0, 1.4436365230870951), (2, 1.6486829480514866)], [(2, 2.7787011975250615), (13, 3.0033073298751716)], [(12, 2.8235119380792368), (11, 3.5328203954650035)], [(2, 4.4383324332626142), (3, 4.7461213833396103)], [(9, 2.2184417808031194), (6, 2.5140945346618748)], [(1, 1.9809805544859667), (4, 2.6563080459021049)], [(8, 0.4179559815851277), (6, 1.3100149006267803)], [(5, 1.3624165615202395), (12, 1.4086653921086838)], [(12, 2.0158293010311836), (8, 2.6064587041503811)], [(5, 0.94077050122789707), (10, 1.1669945530751278)], [(2, 1.3912449088318706), (12, 1.414738064895424)], [(5, 1.1713591488200117), (8, 1.1792202417561299)], [(8, 1.0954533656843817), (3, 1.4560396547432644)], [(6, 1.8172598104596145), (9, 2.3342066656866383)], [(7, 1.3824888993706559), (2, 3.3790800826203178)], [(2, 3.4935979747999126), (10, 3.7048461485462596)], [(10, 3.1644767432538079), (12, 3.2595645029555276)], [(0, 1.0966566440071916), (2, 1.9862070310837052)], [(5, 1.1137674960254296), (14, 1.1745774663058564)], [(9, 2.1970858576152175), (0, 2.3476821058405006)], [(6, 1.9460507435496615), (5, 2.0818930580789332)], [(6, 1.1261904560852432), (9, 1.5643362463674195)], [(7, 1.9402130423925152), (2, 2.0363824804350359)], [(7, 1.856136461056614), (5, 1.9616227736560432)], [(7, 1.0309764369427532), (6, 1.1211327506934055)], [(12, 0.28349313378879454), (8, 0.86088526533542309)], [(5, 1.366653134591201), (1, 1.7245274781573567)], [(3, 0.34432878351677465), (6, 0.38492670257347095)], [(13, 0.2209587080747123), (4, 0.46359885740402046)], [(3, 1.9236455319550496), (10, 1.9528246427320759)], [(10, 1.9773310890541222), (4, 2.0502686155383922)], [(11, 0.47271145284558064), (8, 0.82900413872765766)], [(10, 1.1891841017776441), (7, 2.4051683101740835)], [(10, 0.88196567154963168), (7, 1.1776504654444035)], [(1, 0.23590096307831393), (3, 0.34940098669324626)], [(10, 0.43533080354237574), (7, 0.50801889428246649)], [(3, 1.0178635819643689), (4, 1.2002484143238159)], [(1, 0.82843025884836297), (4, 1.0224239629233922)], [(4, 1.1390995695094681), (14, 1.5357914017539169)], [(4, 0.29237302284345457), (11, 0.29323615064730729)], [(10, 0.1050490679657471), (12, 0.40223512532865058)], [(7, 0.14929148387142208), (14, 0.22286332230744221)], [(13, 0.36813753722803916), (9, 0.39947910144918092)], [(2, 0.35198737107596351), (12, 0.41487581037705684)], [(2, 0.29092972255113536), (4, 0.44154803142010385)], [(14, 1.2585319920183933), (10, 1.2829328283405246)], [(7, 1.6873967855766536), (10, 2.905038337936201)], [(14, 1.2317917659604711), (10, 1.7299251210603959)], [(10, 1.0031259647083757), (9, 1.3130853685821449)], [(10, 0.7217197934267584), (11, 0.82450914023961031)], [(10, 2.7594735376524038), (3, 2.9813544025698828)], [(10, 1.017569823417493), (7, 1.3247886951673922)], [(12, 0.9940680604658052), (8, 1.1505610203838774)], [(13, 1.0664967510299912), (3, 1.2254853769591876)], [(2, 1.3804971453971631), (13, 1.7487532060946458)], [(13, 1.5185319819547349), (14, 3.6151167891307137)], [(10, 2.3225368548270207), (13, 2.6763024051428368)], [(5, 1.3765863993145726), (0, 1.8014982229595282)], [(9, 1.4650924920778305), (3, 2.5687677842671217)], [(11, 1.866991346731413), (9, 2.989442261697111)], [(6, 0.69064227029687442), (0, 1.0384131578192199)], [(7, 0.75415672937832257), (5, 1.6301803366252705)], [(13, 1.5553668559216229), (4, 1.6676903398416383)], [(0, 0.78339923059714978), (1, 0.93036294148561438)], [(7, 2.0266326803023333), (12, 2.2001038051717399)], [(9, 0.65511217140336819), (1, 0.8427191421684681)], [(3, 0.76499577257678109), (12, 1.6203281789882673)], [(14, 0.88342932878773239), (6, 0.92353684862734631)], [(11, 1.6583882434110022), (10, 2.6563936762027018)], [(5, 1.5821293181888538), (2, 1.7765599711728735)], [(13, 1.1572350860905407), (0, 1.4460187384023258)], [(0, 0.23051217057693538), (1, 0.43411576025089821)], [(14, 0.34223099757231312), (6, 1.5497207483588582)], [(6, 0.30093217800212452), (1, 0.67237802916221967)], [(4, 0.29113829011096048), (1, 0.29847935842923834)], [(6, 0.71705873640840456), (1, 0.77262600461565434)], [(13, 0.10212739861027484), (14, 0.29988609040683917)], [(12, 0.94797383619243469), (5, 1.0664995101065886)], [(1, 0.91539408754846419), (8, 1.5958711455497712)], [(9, 0.5474753694847595), (3, 0.85850347632715651)], [(14, 0.64903968911089682), (8, 0.74867940279199374)], [(13, 0.98576748931496005), (12, 1.4429398697215245)], [(14, 0.95313049689271667), (11, 1.0896500415853108)], [(8, 0.73753927838108813), (11, 0.90201006718852439)], [(12, 0.0), (13, 0.0)], [(2, 0.57244940415740886), (1, 0.97197355867544377)], [(2, 1.2427790528475557), (0, 2.2277486959498916)], [(8, 0.86011946802682326), (14, 1.2885842311412234)], [(9, 1.6784765667240749), (0, 1.6842585007944553)], [(2, 0.21530630110083854), (9, 0.24729689436001395)], [(13, 0.14732356171220659), (6, 0.40357113332123223)], [(5, 0.19077514446939278), (8, 0.2453187453950248)], [(9, 0.85081409178577205), (1, 0.93379727012847935)], [(11, 2.4007301338713294), (10, 2.9317423018804281)], [(13, 2.9499037709134646), (9, 3.1449088096131401)], [(12, 1.8280942593541607), (10, 2.0356346433873567)], [(13, 1.8133119272207123), (7, 1.8360634636756228)], [(6, 0.067407197941879785), (9, 0.34984768165172897)], [(3, 0.087090167170735186), (9, 0.098220860574034688)], [(2, 0.28950062395627363), (7, 0.73309235968907416)], [(13, 0.28913942005308702), (2, 0.76768230793104164)], [(4, 0.37884799521782836), (10, 0.70113427961236241)], [(6, 0.66703196471647974), (0, 0.93977698230853923)], [(0, 0.25044697081765888), (12, 0.4577715146740417)], [(8, 0.12589339050277301), (6, 0.53304012438863246)], [(7, 0.18643005913435373), (8, 0.45726227271618824)], [(1, 0.029608061250392916), (4, 0.032092866316121967)], [(2, 0.091016369251592566), (12, 0.1294883014493208)], [(2, 0.089985519238047582), (0, 0.47942281470943549)], [(5, 0.67335250580707573), (3, 1.1106610431688422)], [(1, 0.82318377767875928), (11, 0.84641736062591932)], [(9, 0.35690159670263222), (14, 0.78506373612932767)], [(2, 0.33392484840716846), (3, 0.38976428300904553)], [(3, 1.1547310985269328), (8, 1.2712522461175297)], [(7, 0.044913156621775807), (9, 0.085143664950842662)], [(9, 1.0780425403423877), (11, 2.5159734802403233)], [(3, 0.12889974526583023), (7, 0.23931054368003818)], [(7, 0.3195608959419084), (1, 0.3460591755250188)], [(3, 1.1890763323325941), (4, 1.2187859559838703)], [(13, 0.17069530814528877), (4, 0.59926073540268687)], [(12, 1.2780873791877099), (13, 1.4849227796639022)], [(4, 0.32028927182169753), (2, 0.41932739650866324)], [(7, 1.9471700931802054), (10, 2.4283100953295662)], [(11, 1.4212150662013487), (10, 2.3394294853092679)], [(4, 0.94624943965229069), (10, 1.3487857307874445)], [(11, 1.345328501794836), (7, 1.6708203412059408)], [(7, 1.0514501319119292), (4, 1.5686596102355317)], [(3, 2.564612993581989), (5, 3.7834942193901142)], [(4, 0.91606691413493024), (3, 1.6207241165289956)], [(2, 1.4550050727172374), (5, 2.8400927023484486)], [(0, 0.33958681417017661), (4, 0.59960949277820452)], [(1, 0.81916768392515682), (14, 0.91026046163842356)], [(10, 0.18990931872184225), (3, 0.19655342755813995)], [(11, 0.60778063726100229), (14, 0.71721988429598016)], [(12, 0.60668527144539286), (11, 1.1054059705354813)], [(11, 0.44705517221119279), (0, 0.66206725707619773)], [(13, 0.39239222968974441), (12, 0.44065688917904505)], [(4, 0.40165811659379758), (2, 0.41613537016623431)], [(5, 0.83021005116479207), (14, 1.099290674677502)], [(0, 0.85697954642742658), (5, 0.89559434105972091)], [(2, 0.93351252220544656), (12, 1.0674329448889384)], [(7, 1.5875680199030231), (2, 1.8634442351916309)], [(4, 0.83245709384594446), (0, 0.94412592626741998)], [(3, 0.87940097886208479), (14, 0.88301525825779814)], [(6, 0.64740931685784819), (1, 0.99622169600565258)], [(11, 1.2070857165762465), (0, 1.4757683202402798)], [(8, 1.1042100294390611), (1, 1.5571340218270691)], [(9, 0.57241370031350347), (1, 0.79594059979341159)], [(3, 1.2913611872496902), (11, 1.3492278224981094)], [(0, 1.8534952916445842), (14, 2.2734960917200837)], [(2, 0.71614995591599673), (6, 1.5795425559397893)], [(14, 0.52675954914669942), (0, 0.79586483488569004)], [(9, 0.77349695279082731), (11, 2.4365447587219289)], [(9, 1.0348351568799206), (6, 1.1299365285108425)], [(1, 0.30871139129899605), (3, 0.42511075933554754)], [(2, 0.98545439602764462), (0, 1.1448293684924791)], [(8, 0.61325173201196359), (0, 0.95788495245149685)], [(14, 0.47149254485440267), (0, 0.83219592771175277)], [(2, 0.76657201009956022), (6, 1.4790184806444047)], [(12, 0.51234606716260767), (7, 0.84407091644647758)], [(1, 0.83701883652840248), (4, 0.8777144811712615)], [(6, 0.51189972190892852), (3, 0.51401056239121401)], [(0, 0.80657220202024205), (1, 0.83110072452786377)], [(4, 0.82583766406075865), (13, 0.84724545392437212)], [(12, 0.392261458053909), (8, 0.41154216926530102)], [(12, 0.45525185977779609), (4, 0.57303832739045824)], [(0, 0.58312684999809639), (1, 1.6822189771227491)], [(2, 0.74554216448611377), (12, 0.80555674249190112)], [(8, 0.25881185147007318), (4, 0.32545322271926624)], [(5, 1.3864879401936898), (8, 1.386816025679287)], [(0, 2.5473887820528431), (5, 3.106314428740597)], [(2, 0.6621417935954681), (14, 1.0993848108515911)], [(2, 0.57774296516329471), (5, 3.4861693783067853)], [(10, 0.25908103862785908), (4, 0.26764837292368293)], [(11, 0.6417516365910978), (8, 0.83510594969458696)], [(4, 0.72181910465101995), (12, 1.0956266852195948)], [(12, 1.0452148643258161), (4, 1.0749285814906608)], [(0, 0.30850002626073852), (2, 0.49875622536569292)], [(2, 1.0698798870300907), (10, 1.1845795334126235)], [(1, 1.0774399701099882), (9, 1.7719463191221088)], [(4, 0.57431234363672024), (3, 0.78883540650413031)], [(9, 0.65289888025287846), (8, 1.467341775535695)], [(1, 0.50209539314252849), (2, 0.55502418142517229)], [(3, 0.50326724025504732), (2, 0.62364208634960128)], [(12, 1.2887748129138317), (14, 1.5267094531097793)], [(3, 1.4996352054269309), (5, 1.8876844672840232)], [(4, 0.80200296273343286), (10, 1.9503023497475045)], [(3, 0.98522860910709242), (7, 1.1026524043398713)], [(1, 0.87813965970275143), (6, 2.1774390399634851)], [(7, 0.73477105284464117), (1, 2.2832552897925975)], [(4, 1.8254136911251897), (12, 1.9437264041921398)], [(11, 1.0035722942243306), (5, 1.135379029861457)], [(1, 0.83195108721543298), (14, 0.91895884352130797)], [(3, 0.37614452309636454), (8, 1.0245896703627657)], [(8, 0.46006811224581101), (12, 1.0008966688335923)], [(12, 0.0088764840356388153), (5, 0.12819582452805156)], [(1, 0.0089004142108932253), (11, 0.12328793792034837)], [(1, 0.11229930898563641), (4, 0.1135061588017678)], [(6, 2.2582940331327026), (3, 2.396220203388872)], [(5, 0.46127249189933034), (8, 0.55072977046541349)], [(0, 0.51464170139204823), (6, 0.6431845823321124)], [(14, 0.27193284404553619), (5, 0.43561858225837613)], [(13, 0.43277554399067997), (1, 1.4768571274754283)], [(6, 0.58615726011122682), (11, 0.80062021858823751)], [(7, 0.69679341585681653), (1, 1.2257960220727067)], [(2, 0.4238343497603746), (5, 0.62872382732444576)], [(13, 2.0702253770649492), (12, 2.6809625527812453)], [(0, 0.92260861757862345), (7, 0.99801006592104913)], [(2, 1.7723006178851615), (5, 2.3244855137207399)], [(4, 1.0653848913482937), (5, 1.2750476647812734)], [(11, 1.6340063947639101), (6, 1.657916220981525)], [(8, 0.40291984776966772), (11, 0.42482836474337771)], [(12, 0.69554988258020389), (8, 0.80224633694157088)], [(1, 0.38689191850288046), (4, 2.3255435738057457)], [(12, 1.4408092587284074), (1, 1.4789674074971306)], [(8, 0.89719850788841216), (14, 1.8380702325217944)], [(12, 0.31950199545141722), (8, 0.49753055281137354)], [(14, 0.61356921243542195), (0, 0.91025004953170263)], [(0, 1.6582682199871028), (14, 4.7006559775036658)], [(6, 1.0804496953415084), (1, 3.0802144452195757)], [(0, 0.99150556953527114), (4, 1.1386360226059171)], [(1, 0.4462400416731816), (6, 2.68721450325963)], [(3, 1.1883684554381206), (10, 1.1948127774636894)], [(13, 1.8727399334969743), (2, 2.0389457296197739)], [(8, 0.80340201873922323), (0, 0.95554406672354009)], [(4, 0.41247008325962287), (5, 0.59327508877504109)], [(11, 0.50930950041358358), (6, 0.61943857950366621)], [(0, 0.46590653627949391), (7, 0.54737724596574766)], [(8, 0.45965451873214591), (5, 1.1054712165338276)], [(2, 1.5406276205616372), (5, 1.9757104827764058)], [(4, 0.90560691831880169), (1, 0.93828068692184974)], [(12, 0.52456267942153778), (2, 0.53176838049214126)], [(14, 0.61966481862531531), (3, 0.73944502561536718)], [(5, 0.58781320279940708), (10, 0.96668264620659572)], [(11, 1.5641105488832838), (13, 1.6435841677767478)], [(1, 0.51868753184256911), (0, 0.53928737637076618)], [(12, 1.9220819011038404), (1, 2.3863142238240953)], [(2, 0.32135745552608003), (1, 2.4314907107818566)], [(7, 1.5511362967934834), (10, 2.216252847521353)], [(4, 1.4004654218456656), (2, 2.0152207434296412)], [(13, 0.14998696057686603), (14, 0.42219809294013244)], [(0, 0.97683647227463599), (4, 1.2394686634652428)], [(2, 1.4108755712174943), (7, 2.0460036129867851)], [(13, 1.3358380296630108), (11, 1.7236717119276774)], [(0, 0.49135352630222667), (6, 0.7096308447708628)], [(0, 0.41647586721031121), (5, 0.64899220669951407)], [(6, 1.2286220813834794), (8, 1.6502570681975468)], [(11, 1.2731991602852255), (9, 1.349954897090085)], [(9, 1.4033188750898893), (3, 1.5651235208202079)], [(3, 0.83212299893091779), (5, 1.6796960612773715)], [(0, 0.59683228800817345), (14, 0.70093429282233577)], [(0, 0.43129466707974057), (14, 0.52829466951420812)], [(11, 0.63623952576741782), (9, 0.82325027732960931)], [(5, 0.19104887306934296), (14, 0.29479712287071952)], [(7, 1.0240319916780907), (13, 1.393459613144524)], [(11, 0.93162796502488732), (9, 1.6046865722325852)], [(1, 0.8522952716508968), (9, 1.6621259936014694)], [(3, 0.71663191803099424), (5, 1.0238772709725867)], [(11, 1.5454679046350537), (4, 1.5911176747595079)], [(5, 1.0891491645695561), (1, 1.2387358770279568)], [(4, 0.82720490756782095), (3, 1.1633242038774667)], [(1, 0.38105516143900753), (0, 0.81923417137067545)], [(8, 1.2254860411703448), (9, 1.3024764531184403)], [(0, 0.26395818263582632), (2, 0.30065522753013973)], [(9, 1.2657494554130562), (3, 1.5616931475434099)], [(7, 0.93484250243048383), (5, 1.1301068212167442)], [(5, 1.4916064635523205), (7, 1.6698553480650284)], [(6, 0.76453207750380592), (13, 0.7922613222615964)], [(6, 1.4370173981388292), (3, 2.3925061822150218)], [(0, 1.0189120319974909), (2, 1.1227524817252239)], [(8, 0.97578187837435881), (5, 1.2074242307868417)], [(0, 1.3965365880872882), (8, 1.4381380386933531)]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Beschreibung der Merkmale\n",
    "\n",
    "def get_the_biggest(indexed_features, n):\n",
    "    sorted_list = sorted(indexed_features, key=lambda value: value[1])\n",
    "    biggest = []\n",
    "    for i in range(-(n+1),-1):\n",
    "        biggest.append(sorted_list[i])\n",
    "    return biggest\n",
    "\n",
    "def get_index_list(feature):\n",
    "    indexed_list = []\n",
    "    index = 0\n",
    "    for value in feature:\n",
    "        indexed_list.append((index,value))\n",
    "        index +=1\n",
    "    return indexed_list\n",
    "\n",
    "def find_important_features(matrix, n):\n",
    "    important_feature_list = []\n",
    "    for feature in matrix:\n",
    "        indexed_list = get_index_list(feature)\n",
    "        biggest_six = get_the_biggest(indexed_list, n)\n",
    "        important_feature_list.append(biggest_six)\n",
    "    return important_feature_list\n",
    "\n",
    "\n",
    "print find_important_features(matrix_H, 6)\n",
    "print find_important_features(matrix_W, 2)\n",
    "\n",
    "\n",
    "# Implementierung\n",
    "def showfeatures(W,H,titles,wordvec):\n",
    "    # TODO\n",
    "    return 0\n",
    "\n",
    "print showfeatures(matrix_W,matrix_H,articletitles,allwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diese Methoden dienen nur zu lesbarkeit des Codes\n",
    "def calc_a(A, v):\n",
    "    return np.matmul(A,v)\n",
    "\n",
    "def calc_f(H, v):\n",
    "    return np.matmul(W,v)\n",
    "\n",
    "def calc_a(W, f):\n",
    "    return np.matmul(W,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgaben\n",
    "\n",
    "1. Analysieren Sie die berechneten Topics indem Sie sich überlegen ob die gefundenen 6 Wörter pro Topic wirklich Themen beschreiben.\n",
    "2. Verändern Sie die Parameter der NNMF (Anzahl der Topics $m$, Anzahl der Iterationen). Bei welcher Einstellung der Parameter erhalten Sie das für sie sinnvollste Resultat (sinnvolle Topics)?\n",
    "3. Wie kann die _getwords()_ Methode verbessert werden, so dass noch bedeutsamere Topics gefunden werden? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
